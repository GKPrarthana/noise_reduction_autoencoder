{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6d61471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Prarthana\\\\Desktop\\\\projects\\\\noise_reduction_autoencoder\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7afcf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Prarthana\\\\Desktop\\\\projects\\\\noise_reduction_autoencoder'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b3ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update entity\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainingConfig:\n",
    "    model_dir: Path\n",
    "    image_size: int\n",
    "    epochs: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded2edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update configurationmanager\n",
    "from src import logger\n",
    "from src.utils.common import read_yaml, create_directories\n",
    "#from src.entity.config_entity import DataIngestionConfig, DataSplittingConfig, DataPreprocessingConfig, ModelTrainingConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=\"config/config.yaml\", params_filepath=\"params.yaml\"):\n",
    "        self.config = read_yaml(Path(config_filepath))\n",
    "        self.params = read_yaml(Path(params_filepath))\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        config = self.config.model_training\n",
    "        create_directories([config.model_dir])\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            model_dir=config.model_dir,\n",
    "            image_size=config.image_size,\n",
    "            epochs=config.epochs\n",
    "        )\n",
    "        return model_training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f43b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update components\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from src import logger\n",
    "#from src.entity.config_entity import ModelTrainingConfig\n",
    "\n",
    "class ModelTraining:\n",
    "    def __init__(self, config: ModelTrainingConfig, datasets: dict):\n",
    "        self.config = config\n",
    "        self.datasets = datasets\n",
    "        self.model = None\n",
    "\n",
    "    def build_autoencoder(self):\n",
    "        \"\"\"Build a convolutional autoencoder for denoising.\"\"\"\n",
    "        input_shape = (self.config.image_size, self.config.image_size, 1)  # Grayscale images\n",
    "        \n",
    "        # Encoder\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "        x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "        x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "        encoded = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(encoded)\n",
    "        x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "        x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "        x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "        decoded = tf.keras.layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "        # Autoencoder model\n",
    "        self.model = tf.keras.Model(inputs, decoded)\n",
    "        self.model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "        self.model.summary(print_fn=lambda x: logger.info(x))\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the autoencoder using the preprocessed datasets.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting model training\")\n",
    "            \n",
    "            # Build the model\n",
    "            self.build_autoencoder()\n",
    "            \n",
    "            # Train the model\n",
    "            history = self.model.fit(\n",
    "                self.datasets[\"train\"],\n",
    "                validation_data=self.datasets[\"val\"],\n",
    "                epochs=self.config.epochs,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor=\"val_loss\",\n",
    "                        patience=5,\n",
    "                        restore_best_weights=True\n",
    "                    ),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(\n",
    "                        str(self.config.model_dir / \"autoencoder_best.h5\"),\n",
    "                        monitor=\"val_loss\",\n",
    "                        save_best_only=True\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_loss = self.model.evaluate(self.datasets[\"test\"])\n",
    "            logger.info(f\"Test loss (MSE): {test_loss}\")\n",
    "            \n",
    "            # Save the final model\n",
    "            self.model.save(self.config.model_dir / \"autoencoder_final.h5\")\n",
    "            logger.info(f\"Model saved to {self.config.model_dir / 'autoencoder_final.h5'}\")\n",
    "            \n",
    "            logger.info(\"Model training completed\")\n",
    "            return history\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model training failed: {e}\", exc_info=True)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c634f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:03:52,053: INFO: common: YAML file: config\\config.yaml loaded successfully]\n",
      "[2025-04-23 23:03:52,058: INFO: common: YAML file: params.yaml loaded successfully]\n",
      "[2025-04-23 23:03:52,060: INFO: common: Created directory at: artifacts]\n",
      "[2025-04-23 23:03:52,062: INFO: common: Created directory at: artifacts/models]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     history = model_training.train()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     config = ConfigurationManager()\n\u001b[32m      6\u001b[39m     model_training_config = config.get_model_training_config()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     model_training = ModelTraining(config=model_training_config, datasets=\u001b[38;5;28mself\u001b[39m.datasets)\n\u001b[32m      8\u001b[39m     history = model_training.train()\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameError\u001b[39m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "#update pipeline\n",
    "\n",
    "try:\n",
    "    # Initialize and train the model\n",
    "    config = ConfigurationManager()\n",
    "    model_training_config = config.get_model_training_config()\n",
    "    model_training = ModelTraining(config=model_training_config, datasets=self.datasets)\n",
    "    history = model_training.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f24b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3080dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf69d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:38:58,937: INFO: common: YAML file: c:\\Users\\Prarthana\\Desktop\\projects\\noise_reduction_autoencoder\\config\\config.yaml loaded successfully]\n",
      "[2025-04-23 23:38:58,940: INFO: common: YAML file: c:\\Users\\Prarthana\\Desktop\\projects\\noise_reduction_autoencoder\\params.yaml loaded successfully]\n",
      "[2025-04-23 23:38:58,942: INFO: common: Created directory at: artifacts]\n",
      "[2025-04-23 23:38:58,942: INFO: data_preprocessing: Starting data preprocessing]\n",
      "[2025-04-23 23:38:58,953: INFO: data_preprocessing: train set: 436 clean images, 436 noisy images]\n",
      "[2025-04-23 23:38:59,052: INFO: data_preprocessing: val set: 93 clean images, 93 noisy images]\n",
      "[2025-04-23 23:38:59,068: INFO: data_preprocessing: test set: 95 clean images, 95 noisy images]\n",
      "[2025-04-23 23:38:59,084: INFO: data_preprocessing: Data preprocessing completed]\n",
      "[2025-04-23 23:38:59,086: INFO: common: Created directory at: artifacts/models]\n",
      "[2025-04-23 23:38:59,087: INFO: model_training: Starting model training]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:38:59,161: INFO: model_training: Model: \"functional\"\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
      "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
      "│ input_layer (InputLayer)        │ (None, 128, 128, 1)    │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d (Conv2D)                 │ (None, 128, 128, 32)   │           320 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d (MaxPooling2D)    │ (None, 64, 64, 32)     │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d_1 (Conv2D)               │ (None, 64, 64, 64)     │        18,496 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ max_pooling2d_1 (MaxPooling2D)  │ (None, 32, 32, 64)     │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d_2 (Conv2D)               │ (None, 32, 32, 128)    │        73,856 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d_3 (Conv2D)               │ (None, 32, 32, 64)     │        73,792 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ up_sampling2d (UpSampling2D)    │ (None, 64, 64, 64)     │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d_4 (Conv2D)               │ (None, 64, 64, 32)     │        18,464 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ up_sampling2d_1 (UpSampling2D)  │ (None, 128, 128, 32)   │             0 │\n",
      "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
      "│ conv2d_5 (Conv2D)               │ (None, 128, 128, 1)    │           289 │\n",
      "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      " Total params: 185,217 (723.50 KB)\n",
      " Trainable params: 185,217 (723.50 KB)\n",
      " Non-trainable params: 0 (0.00 B)\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512ms/step - loss: 0.0459"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:39:09,127: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 607ms/step - loss: 0.0451 - val_loss: 0.0149\n",
      "Epoch 2/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844ms/step - loss: 0.0115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:39:23,224: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 928ms/step - loss: 0.0114 - val_loss: 0.0069\n",
      "Epoch 3/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752ms/step - loss: 0.0059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:39:36,419: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 836ms/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 4/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 701ms/step - loss: 0.0046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:39:48,943: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 780ms/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 5/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 782ms/step - loss: 0.0039 - val_loss: 0.0058\n",
      "Epoch 6/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667ms/step - loss: 0.0046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:40:12,679: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 746ms/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 7/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684ms/step - loss: 0.0035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:40:24,602: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 765ms/step - loss: 0.0035 - val_loss: 0.0033\n",
      "Epoch 8/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686ms/step - loss: 0.0031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:40:36,645: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 768ms/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 9/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - loss: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:40:48,488: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 767ms/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 10/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 766ms/step - loss: 0.0029 - val_loss: 0.0035\n",
      "Epoch 11/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717ms/step - loss: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:41:12,903: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 801ms/step - loss: 0.0030 - val_loss: 0.0026\n",
      "Epoch 12/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 776ms/step - loss: 0.0026 - val_loss: 0.0027\n",
      "Epoch 13/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 748ms/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 14/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670ms/step - loss: 0.0025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:41:48,672: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 747ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 15/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 681ms/step - loss: 0.0023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:42:00,552: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 764ms/step - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 16/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 766ms/step - loss: 0.0025 - val_loss: 0.0023\n",
      "Epoch 17/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694ms/step - loss: 0.0023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:42:24,630: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 770ms/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 18/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - loss: 0.0022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:42:36,702: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 771ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 19/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 687ms/step - loss: 0.0022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:42:48,629: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 765ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 778ms/step - loss: 0.0022 - val_loss: 0.0033\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-23 23:43:01,933: INFO: model_training: Test loss (MSE): 0.0020507618319243193]\n",
      "[2025-04-23 23:43:01,936: WARNING: saving_api: You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. ]\n",
      "[2025-04-23 23:43:02,051: INFO: model_training: Model saved to artifacts\\models\\autoencoder_final.h5]\n",
      "[2025-04-23 23:43:02,053: INFO: model_training: Model training completed]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss plot saved to artifacts/plots/training_loss.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.pipeline.stage_05_model_training import ModelTrainingPipeline\n",
    "\n",
    "obj = ModelTrainingPipeline()\n",
    "history = obj.main()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"docs/plots/training_loss.png\")\n",
    "plt.close()\n",
    "print(\"Loss plot saved to docs/plots/training_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61b08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denoise_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
